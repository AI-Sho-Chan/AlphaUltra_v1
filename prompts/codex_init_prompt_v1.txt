You are Codex CLI (Executor). Implement robust data/ETL for AlphaUltra under these non-negotiable rules:

GOALS
- Build end-to-end pipeline for JP TDNET first: crawl (Kabutan) → normalize (JSON) → features_tdnet.parquet → prices (Stooq/YF) → T+1 as-of merge → healthcheck.
- Target later: EDINET/EDGAR and US expansion.
- Optimize for reliability and repeatability, not for ML design decisions.

HARD RULES (from project constitution)
- Leakage ban; JST/T+1 only; merge_asof(direction="forward") or searchsorted to next trading day.
- Purged/Embargo CV, cluster purge, WF + DSR, isotonic → PPV≥80% (Architect owns modeling).
- v1 path only: C:\AI\AlphaUltra_v1\...
- UTF-8 everywhere. Always create CSV even when 0 rows; on 0 rows save debug HTML.

DELIVERABLES (create these scripts with idempotent CLIs; all UTF-8; write under v1)
1) tools/kabu_crawl_bs.py
   - Input: --start YYYY-MM-DD --end YYYY-MM-DD --sleep 0.3 --max-pages 0
   - Output: data/raw/kabutan/YYYY-MM-DD/tdnet.csv (headers: コード,掲載日時,タイトル,URL)
   - Logic: requests + BeautifulSoup; parse both new [data-code] cards and legacy table rows; page loop until empty/duplicate; save debug HTML on empty.
2) scripts/csv2json_bulk.py
   - Walk data/raw/kabutan/**/tdnet.csv → data/raw/tdnet/YYYY/MM/DD/*.json
   - Fields: {ticker:"####.T", code4, title, published_at_jst, date, event_type:"other", [url_detail],[url_pdf]}
   - Strict code extraction from code column only (no 4-digit year mis-parse).
3) scripts/tdnet_features.py
   - Read raw/tdnet, params.start_date/end_date.
   - Emit features_tdnet/tdnet_event_features.parquet with event_type one-hot, polarity flags, placeholders for strength/novelty, and eff_date (BDay+1 normalized).
4) scripts/prices_jp_fetch_bulk.py
   - For all JP tickers in features_tdnet: Stooq first (s=####.jp), then YF fallback.
   - Save raw/prices/*.parquet, then call price_std_build.py.
5) scripts/price_std_build.py
   - Merge all raw/prices/* → proc/prices/jp_prices_std.parquet
   - Columns: ticker,date,adj_close,volume. Drop duplicates; normalize dates.
6) scripts/tdnet_align_join_next_trading.py
   - searchsorted(next trading day) per ticker to set eff_date; merge prices; write proc/dataset/tdnet_panel.parquet
7) scripts/healthcheck_tdnet_fast_v2.py
   - JSON print: panel_shape, tickers, price_coverage, date_min/max.
   - Coverage uses merge with prices (suffix tolerant: adj_close | adj_close_px | adj_close_x/y).
8) scripts/run_wave0.ps1
   - Year loop: crawl → csv2json_bulk → tdnet_features(start/end) → prices → price_std → align → healthcheck.
   - Print existence checks (3 lines) and tail 20 lines of logs.

CONSTRAINTS
- Never write to C:\AI\AlphaUltra\... (legacy). v1 only.
- On any failure: save debug artifacts and continue next item. No silent exits.

SUCCESS CRITERIA (Wave-0)
- features_rows ≥ 30k, dataset coverage > 0.95, date_min ≤ 2013-09-01.
- healthcheck JSON printed to stdout; existence checks: 1) raw tdnet json count, 2) features file exists, 3) panel file exists.

REPORTING FORMAT (every run)
- Echo: commit SHA (if git present), tail 20 lines of each log, and existence checks (3 lines).

DO NOT
- Change modeling rules or thresholds; Architect owns those. You only implement reliable data plumbing.

Now start by generating tools/kabu_crawl_bs.py with the DOM selectors described, then csv2json_bulk.py, then the rest. Use UTF-8, v1 paths, and create missing directories.
